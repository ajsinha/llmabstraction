# LLM Abstraction System Configuration
# Â© 2025-2030 All rights reserved Ashutosh Sinha
# email: ajsinha@gmail.com

# Application Settings
app.name=LLM Abstraction System
app.version=1.0.0
app.environment=development

# Logging Configuration
logging.level=INFO
logging.format=%(asctime)s - %(name)s - %(levelname)s - %(message)s
logging.file=logs/llm_abstraction.log

# Default Provider and Model
# These can be overridden by configuration JSON or at runtime
default.provider=mock
default.model=mock-model
default.history_size=50
fallback_to_mock=true

# API Keys (can also be set via environment variables)
# Format: <PROVIDER>_API_KEY
# Example: ANTHROPIC_API_KEY=sk-ant-xxxxx
# Leave empty to use environment variables

# Anthropic
# ANTHROPIC_API_KEY=

# Google AI
# GOOGLE_API_KEY=

# Together AI
# TOGETHER_API_KEY=

# Hugging Face
# HUGGINGFACE_API_KEY=

# Grok (xAI)
# GROK_API_KEY=

# AWS Bedrock Configuration
# Uses AWS credentials from environment or AWS CLI configuration
aws.bedrock.region=us-east-1
aws.bedrock.profile=default

# Performance Settings
max_retries=3
timeout_seconds=60
rate_limit_requests_per_minute=60

# Cache Settings
enable_response_cache=false
cache_ttl_seconds=3600
cache_max_size=1000

# Safety Settings
enable_content_filtering=true
max_prompt_length=100000
max_response_length=100000

# Monitoring
enable_metrics=true
metrics_export_interval_seconds=60

# Development Settings
debug_mode=false
verbose_logging=false
save_request_logs=false
