{
  "defaults": {
    "provider": "mock",
    "model": "mock-model",
    "history_size": 50,
    "fallback_to_mock": true
  },
  "providers": {
    "mock": {
      "name": "mock",
      "description": "Mock provider for testing without real API calls",
      "enabled": true,
      "requires_api_key": false,
      "models": ["mock-model", "mock-model-large", "mock-model-fast"]
    },
    "anthropic": {
      "name": "anthropic",
      "description": "Anthropic Claude models",
      "enabled": true,
      "requires_api_key": true,
      "api_key_env": "ANTHROPIC_API_KEY",
      "website": "https://www.anthropic.com",
      "models": [
        "claude-3-5-sonnet-20241022",
        "claude-3-opus-20240229",
        "claude-3-sonnet-20240229",
        "claude-3-haiku-20240307"
      ]
    },
    "bedrock": {
      "name": "bedrock",
      "description": "AWS Bedrock - Multiple model providers via AWS",
      "enabled": true,
      "requires_api_key": false,
      "requires_aws_credentials": true,
      "default_region": "us-east-1",
      "website": "https://aws.amazon.com/bedrock/",
      "models": [
        "meta.llama3-3-70b-instruct-v1:0",
        "meta.llama3-1-405b-instruct-v1:0",
        "meta.llama3-1-70b-instruct-v1:0",
        "meta.llama3-1-8b-instruct-v1:0",
        "amazon.titan-text-premier-v1:0",
        "amazon.titan-text-express-v1",
        "anthropic.claude-3-5-sonnet-20241022-v2:0",
        "anthropic.claude-3-opus-20240229-v1:0",
        "mistral.mistral-large-2407-v1:0",
        "mistral.mixtral-8x7b-instruct-v0:1"
      ]
    },
    "together": {
      "name": "together",
      "description": "Together AI - Fast inference for open source models",
      "enabled": true,
      "requires_api_key": true,
      "api_key_env": "TOGETHER_API_KEY",
      "website": "https://www.together.ai",
      "models": [
        "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo",
        "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
        "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
        "mistralai/Mixtral-8x7B-Instruct-v0.1",
        "mistralai/Mistral-7B-Instruct-v0.2",
        "Qwen/Qwen2.5-72B-Instruct-Turbo",
        "deepseek-ai/deepseek-llm-67b-chat"
      ]
    },
    "google": {
      "name": "google",
      "description": "Google AI - Gemini models",
      "enabled": true,
      "requires_api_key": true,
      "api_key_env": "GOOGLE_API_KEY",
      "website": "https://ai.google.dev",
      "models": [
        "gemini-1.5-pro-latest",
        "gemini-1.5-flash-latest",
        "gemini-1.0-pro",
        "gemini-pro",
        "gemini-1.5-flash-8b"
      ]
    },
    "huggingface": {
      "name": "huggingface",
      "description": "Hugging Face Inference API",
      "enabled": true,
      "requires_api_key": true,
      "api_key_env": "HUGGINGFACE_API_KEY",
      "website": "https://huggingface.co",
      "models": [
        "meta-llama/Meta-Llama-3.1-70B-Instruct",
        "meta-llama/Meta-Llama-3.1-8B-Instruct",
        "mistralai/Mistral-7B-Instruct-v0.3",
        "mistralai/Mixtral-8x7B-Instruct-v0.1",
        "microsoft/Phi-3-mini-4k-instruct",
        "Qwen/Qwen2.5-72B-Instruct",
        "google/gemma-2-9b-it",
        "tiiuae/falcon-180B-chat"
      ]
    },
    "grok": {
      "name": "grok",
      "description": "xAI Grok models",
      "enabled": true,
      "requires_api_key": true,
      "api_key_env": "GROK_API_KEY",
      "api_base": "https://api.x.ai/v1",
      "website": "https://x.ai",
      "models": [
        "grok-beta",
        "grok-vision-beta"
      ]
    }
  },
  "models": {
    "mock-model": {
      "provider": "mock",
      "name": "mock-model",
      "description": "Basic mock model for testing",
      "version": "1.0",
      "context_window": 4096,
      "max_output_tokens": 4096,
      "strengths": ["Testing", "Development", "Prototyping"],
      "use_cases": ["Testing", "Development"],
      "cost_per_1m_input_tokens": 0.0,
      "cost_per_1m_output_tokens": 0.0,
      "free_tier": true
    },
    "claude-3-5-sonnet-20241022": {
      "provider": "anthropic",
      "name": "claude-3-5-sonnet-20241022",
      "description": "Most intelligent Claude model, balancing intelligence with speed",
      "version": "3.5",
      "context_window": 200000,
      "max_output_tokens": 8192,
      "strengths": ["Complex reasoning", "Code generation", "Analysis", "Research"],
      "use_cases": ["Complex tasks", "Code generation", "Research", "Analysis"],
      "cost_per_1m_input_tokens": 3.00,
      "cost_per_1m_output_tokens": 15.00,
      "free_tier": false
    },
    "claude-3-opus-20240229": {
      "provider": "anthropic",
      "name": "claude-3-opus-20240229",
      "description": "Most powerful Claude model for complex tasks",
      "version": "3.0",
      "context_window": 200000,
      "max_output_tokens": 4096,
      "strengths": ["Complex analysis", "Research", "Advanced reasoning", "Creative writing"],
      "use_cases": ["Research", "Complex analysis", "Creative projects"],
      "cost_per_1m_input_tokens": 15.00,
      "cost_per_1m_output_tokens": 75.00,
      "free_tier": false
    },
    "claude-3-haiku-20240307": {
      "provider": "anthropic",
      "name": "claude-3-haiku-20240307",
      "description": "Fastest Claude model for simple tasks",
      "version": "3.0",
      "context_window": 200000,
      "max_output_tokens": 4096,
      "strengths": ["Speed", "Low cost", "Simple tasks", "Chat"],
      "use_cases": ["Simple tasks", "Chat", "High-volume processing"],
      "cost_per_1m_input_tokens": 0.25,
      "cost_per_1m_output_tokens": 1.25,
      "free_tier": false
    },
    "meta.llama3-3-70b-instruct-v1:0": {
      "provider": "bedrock",
      "name": "meta.llama3-3-70b-instruct-v1:0",
      "description": "Meta Llama 3.3 70B - Latest Llama model",
      "version": "3.3",
      "context_window": 8192,
      "max_output_tokens": 8192,
      "strengths": ["General purpose", "Instruction following", "Multi-lingual", "Open source"],
      "use_cases": ["General purpose", "Multi-lingual tasks", "Instruction following"],
      "cost_per_1m_input_tokens": 0.99,
      "cost_per_1m_output_tokens": 0.99,
      "free_tier": false
    },
    "meta.llama3-1-405b-instruct-v1:0": {
      "provider": "bedrock",
      "name": "meta.llama3-1-405b-instruct-v1:0",
      "description": "Meta Llama 3.1 405B - Largest Llama model",
      "version": "3.1",
      "context_window": 32768,
      "max_output_tokens": 32768,
      "strengths": ["Complex reasoning", "Code generation", "Extended context", "Advanced tasks"],
      "use_cases": ["Complex reasoning", "Long-context tasks", "Code generation"],
      "cost_per_1m_input_tokens": 5.32,
      "cost_per_1m_output_tokens": 16.00,
      "free_tier": false
    },
    "meta.llama3-1-70b-instruct-v1:0": {
      "provider": "bedrock",
      "name": "meta.llama3-1-70b-instruct-v1:0",
      "description": "Meta Llama 3.1 70B - Balanced performance",
      "version": "3.1",
      "context_window": 32768,
      "max_output_tokens": 32768,
      "strengths": ["Balanced performance", "Cost-effective", "Long context", "Reliable"],
      "use_cases": ["General purpose", "Cost-effective tasks", "Long-context processing"],
      "cost_per_1m_input_tokens": 0.99,
      "cost_per_1m_output_tokens": 0.99,
      "free_tier": false
    },
    "meta.llama3-1-8b-instruct-v1:0": {
      "provider": "bedrock",
      "name": "meta.llama3-1-8b-instruct-v1:0",
      "description": "Meta Llama 3.1 8B - Fast and efficient",
      "version": "3.1",
      "context_window": 32768,
      "max_output_tokens": 32768,
      "strengths": ["Fast inference", "Low cost", "Simple tasks", "High volume"],
      "use_cases": ["Simple tasks", "High-volume processing", "Cost-sensitive applications"],
      "cost_per_1m_input_tokens": 0.22,
      "cost_per_1m_output_tokens": 0.22,
      "free_tier": false
    },
    "gemini-1.5-pro-latest": {
      "provider": "google",
      "name": "gemini-1.5-pro-latest",
      "description": "Most capable Gemini model for complex tasks",
      "version": "1.5",
      "context_window": 2000000,
      "max_output_tokens": 8192,
      "strengths": ["Long context", "Complex reasoning", "Multimodal", "Research"],
      "use_cases": ["Long-context analysis", "Complex reasoning", "Multimodal tasks"],
      "cost_per_1m_input_tokens": 1.25,
      "cost_per_1m_output_tokens": 5.00,
      "free_tier": false
    },
    "gemini-1.5-flash-latest": {
      "provider": "google",
      "name": "gemini-1.5-flash-latest",
      "description": "Fast and efficient Gemini model",
      "version": "1.5",
      "context_window": 1000000,
      "max_output_tokens": 8192,
      "strengths": ["Speed", "Cost-effective", "Long context", "Multimodal"],
      "use_cases": ["Fast processing", "Cost-effective tasks", "High-volume applications"],
      "cost_per_1m_input_tokens": 0.075,
      "cost_per_1m_output_tokens": 0.30,
      "free_tier": true
    },
    "gemini-1.5-flash-8b": {
      "provider": "google",
      "name": "gemini-1.5-flash-8b",
      "description": "Ultra-fast and low-cost Gemini model",
      "version": "1.5",
      "context_window": 1000000,
      "max_output_tokens": 8192,
      "strengths": ["Ultra-fast", "Very low cost", "High volume", "Long context"],
      "use_cases": ["High-volume processing", "Cost-sensitive applications", "Simple tasks"],
      "cost_per_1m_input_tokens": 0.0375,
      "cost_per_1m_output_tokens": 0.15,
      "free_tier": true
    },
    "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo": {
      "provider": "together",
      "name": "meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo",
      "description": "Largest Llama model via Together AI",
      "version": "3.1",
      "context_window": 16384,
      "max_output_tokens": 16384,
      "strengths": ["Complex reasoning", "Code generation", "Open source", "Advanced tasks"],
      "use_cases": ["Complex reasoning", "Code generation", "Advanced analysis"],
      "cost_per_1m_input_tokens": 3.50,
      "cost_per_1m_output_tokens": 3.50,
      "free_tier": false
    },
    "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo": {
      "provider": "together",
      "name": "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
      "description": "Balanced Llama model via Together AI",
      "version": "3.1",
      "context_window": 32768,
      "max_output_tokens": 32768,
      "strengths": ["Balanced performance", "Fast", "Cost-effective", "Reliable"],
      "use_cases": ["General purpose", "Balanced performance", "Cost-effective tasks"],
      "cost_per_1m_input_tokens": 0.88,
      "cost_per_1m_output_tokens": 0.88,
      "free_tier": false
    },
    "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo": {
      "provider": "together",
      "name": "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
      "description": "Fast and efficient Llama model",
      "version": "3.1",
      "context_window": 8192,
      "max_output_tokens": 8192,
      "strengths": ["Speed", "Low cost", "Simple tasks", "High volume"],
      "use_cases": ["Simple tasks", "High-volume processing", "Cost-sensitive applications"],
      "cost_per_1m_input_tokens": 0.18,
      "cost_per_1m_output_tokens": 0.18,
      "free_tier": false
    },
    "mistralai/Mixtral-8x7B-Instruct-v0.1": {
      "provider": "together",
      "name": "mistralai/Mixtral-8x7B-Instruct-v0.1",
      "description": "Mixtral MoE model",
      "version": "0.1",
      "context_window": 32768,
      "max_output_tokens": 32768,
      "strengths": ["MoE architecture", "Multi-lingual", "Code", "Efficient"],
      "use_cases": ["Multi-lingual tasks", "Code generation", "General purpose"],
      "cost_per_1m_input_tokens": 0.60,
      "cost_per_1m_output_tokens": 0.60,
      "free_tier": false
    },
    "Qwen/Qwen2.5-72B-Instruct-Turbo": {
      "provider": "together",
      "name": "Qwen/Qwen2.5-72B-Instruct-Turbo",
      "description": "Qwen 2.5 72B - Strong multilingual model",
      "version": "2.5",
      "context_window": 32768,
      "max_output_tokens": 32768,
      "strengths": ["Multilingual", "Math", "Code", "Reasoning"],
      "use_cases": ["Multilingual tasks", "Math problems", "Code generation"],
      "cost_per_1m_input_tokens": 0.88,
      "cost_per_1m_output_tokens": 0.88,
      "free_tier": false
    },
    "grok-beta": {
      "provider": "grok",
      "name": "grok-beta",
      "description": "Grok conversational AI model",
      "version": "beta",
      "context_window": 131072,
      "max_output_tokens": 131072,
      "strengths": ["Real-time info", "Conversational", "X platform integration", "Current events"],
      "use_cases": ["Conversational AI", "Real-time information", "Current events"],
      "cost_per_1m_input_tokens": 5.00,
      "cost_per_1m_output_tokens": 15.00,
      "free_tier": false
    }
  }
}
